{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import glob\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision import transforms as t\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/jbenn/data/hymenoptera/\"\n",
    "PHASES = ['train', 'val']\n",
    "MEAN_TRANSFORM = np.array([0.485, 0.456, 0.406])\n",
    "STD_TRANSFORM = np.array([0.229, 0.224, 0.225])\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "transforms = { \n",
    "    'train': t.Compose([\n",
    "        t.RandomSizedCrop(224),\n",
    "        t.RandomHorizontalFlip(),\n",
    "        t.ToTensor(),\n",
    "#         t.Normalize(MEAN_TRANSFORM, STD_TRANSFORM)\n",
    "    ]),\n",
    "    'val': t.Compose([\n",
    "        t.Scale(256),\n",
    "        t.CenterCrop(224),\n",
    "        t.ToTensor(),\n",
    "#         t.Normalize(MEAN_TRANSFORM, STD_TRANSFORM)\n",
    "    ])\n",
    "}\n",
    "    \n",
    "image_folders = { \n",
    "    phase: datasets.ImageFolder(DATA_PATH + phase, transforms[phase]) \n",
    "    for phase in PHASES \n",
    "}\n",
    "\n",
    "class_names = image_folders['val'].classes\n",
    "\n",
    "dataloaders = { phase: torch.utils.data.DataLoader(\n",
    "        dataset=image_folders[phase],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    ) for phase in PHASES }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def imshow(inp):\n",
    "#     inp = inp.numpy().transpose((1, 2, 0))\n",
    "#     mean = np.array([0.485, 0.456, 0.406])\n",
    "#     std = np.array([0.229, 0.224, 0.225])\n",
    "#     inp = std * inp + mean\n",
    "#     plt.imshow((inp * 255).numpy().transpose(1, 2, 0).astype(\"uint8\"))\n",
    "#     print(inp * 255)\n",
    "#     if title is not None:\n",
    "#         plt.title(title)\n",
    "#     plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "# inputs, classes = next(iter(dataloaders['train']))\n",
    "# grid = torchvision.utils.make_grid(inputs)\n",
    "# ValueError: Floating point image RGB values must be in the 0..1 range.\n",
    "# imshow(grid, title=[class_names[x] for x in classes])\n",
    "# imshow(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_sizes = { phase: len(image_folders[phase]) for phase in PHASES }\n",
    "\n",
    "def train(model, criterion, optimizer, num_epochs):\n",
    "    loss_history = { 'train': [], 'val': [] }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in PHASES:\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            elif phase == 'val':\n",
    "                model.train(False)\n",
    "            \n",
    "            for inputs, classes in dataloaders[phase]:\n",
    "                inputs = Variable(inputs.cuda(1))\n",
    "                classes = Variable(classes.cuda(1))\n",
    "                \n",
    "                model.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                \n",
    "                loss = criterion(outputs, classes)\n",
    "\n",
    "                loss_history[phase].append(loss.data[0])\n",
    "                running_loss += loss.data[0]\n",
    "                running_corrects += torch.sum(preds == classes.data)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            print(\"{} {}  \\tloss: {:.4f}\\tacc: {:.4f}\".format(epoch, phase, epoch_loss, epoch_acc))\n",
    "\n",
    "    torch.save(model.state_dict(), \"last_weights\")\n",
    "    return loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KERNEL_SIZE = 3\n",
    "\n",
    "class VGGish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, KERNEL_SIZE, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, KERNEL_SIZE, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, KERNEL_SIZE, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, KERNEL_SIZE, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, KERNEL_SIZE, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, KERNEL_SIZE, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Linear(in_features=256*56*56, out_features=2)\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        out = self.layer1(inp)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return self.softmax(out)\n",
    "\n",
    "model = VGGish().cuda(1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=.0001, momentum=0.9)\n",
    "loss_history = train(model, criterion, optimizer, num_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualizer\n",
    "# enumerate val\n",
    "# wrap vals in var, cudafy\n",
    "# get predictions\n",
    "# plt.subplot\n",
    "# imshow\n",
    "plt.plot(loss_history['train'])\n",
    "plt.plot(loss_history['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train  \tloss: 0.0851\tacc: 0.5820\n",
      "0 val  \tloss: 0.0865\tacc: 0.5817\n",
      "1 train  \tloss: 0.0833\tacc: 0.5984\n",
      "1 val  \tloss: 0.0873\tacc: 0.5882\n",
      "2 train  \tloss: 0.0807\tacc: 0.6475\n",
      "2 val  \tloss: 0.0910\tacc: 0.5425\n",
      "3 train  \tloss: 0.0785\tacc: 0.6598\n",
      "3 val  \tloss: 0.0780\tacc: 0.6601\n",
      "4 train  \tloss: 0.0793\tacc: 0.6516\n",
      "4 val  \tloss: 0.0878\tacc: 0.5686\n",
      "5 train  \tloss: 0.0795\tacc: 0.6844\n",
      "5 val  \tloss: 0.0840\tacc: 0.6732\n",
      "6 train  \tloss: 0.0797\tacc: 0.6598\n",
      "6 val  \tloss: 0.0796\tacc: 0.6667\n",
      "7 train  \tloss: 0.0794\tacc: 0.6639\n",
      "7 val  \tloss: 0.0898\tacc: 0.6209\n",
      "8 train  \tloss: 0.0801\tacc: 0.6475\n",
      "8 val  \tloss: 0.0785\tacc: 0.6732\n",
      "9 train  \tloss: 0.0792\tacc: 0.6516\n",
      "9 val  \tloss: 0.0764\tacc: 0.6732\n",
      "10 train  \tloss: 0.0762\tacc: 0.7172\n",
      "10 val  \tloss: 0.0801\tacc: 0.6928\n",
      "11 train  \tloss: 0.0834\tacc: 0.6189\n",
      "11 val  \tloss: 0.0833\tacc: 0.6601\n",
      "12 train  \tloss: 0.0814\tacc: 0.6516\n",
      "12 val  \tloss: 0.0808\tacc: 0.6471\n",
      "13 train  \tloss: 0.0806\tacc: 0.6598\n",
      "13 val  \tloss: 0.0891\tacc: 0.5882\n",
      "14 train  \tloss: 0.0771\tacc: 0.6926\n",
      "14 val  \tloss: 0.0924\tacc: 0.5752\n",
      "15 train  \tloss: 0.0773\tacc: 0.6598\n",
      "15 val  \tloss: 0.0836\tacc: 0.6536\n",
      "16 train  \tloss: 0.0759\tacc: 0.7008\n",
      "16 val  \tloss: 0.0812\tacc: 0.6405\n",
      "17 train  \tloss: 0.0763\tacc: 0.6967\n"
     ]
    }
   ],
   "source": [
    "KERNEL_SIZE = 3\n",
    "\n",
    "class VGGish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, KERNEL_SIZE, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, KERNEL_SIZE, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, KERNEL_SIZE, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, KERNEL_SIZE, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, KERNEL_SIZE, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, KERNEL_SIZE, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer_fc = nn.Sequential(\n",
    "            nn.Linear(in_features=256*56*56, out_features=200),\n",
    "            nn.BatchNorm2d(200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=200, out_features=200),\n",
    "            nn.BatchNorm2d(200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=200, out_features=2)\n",
    "        )\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        out = self.layer1(inp)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.layer_fc(out)\n",
    "        return self.softmax(out)\n",
    "\n",
    "adam_model = VGGish().cuda(1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "adam_optimizer = optim.Adam(adam_model.parameters())\n",
    "adam_loss_history = train(adam_model, criterion, adam_optimizer, num_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
